{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "import torch\n",
        "import os\n",
        "from PIL import Image\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "categories = [\n",
        "        {\n",
        "            \"id\": 1,\n",
        "            \"name\": \"person\",\n",
        "            \"supercategory\": \"\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": 2,\n",
        "            \"name\": \"bicycle\",\n",
        "            \"supercategory\": \"\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": 3,\n",
        "            \"name\": \"car\",\n",
        "            \"supercategory\": \"\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": 4,\n",
        "            \"name\": \"motorcycle\",\n",
        "            \"supercategory\": \"\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": 6,\n",
        "            \"name\": \"bus\",\n",
        "            \"supercategory\": \"\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": 8,\n",
        "            \"name\": \"truck\",\n",
        "            \"supercategory\": \"\"\n",
        "        }\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>DETR</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\anaconda3\\envs\\detr\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
            "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "c:\\Users\\User\\anaconda3\\envs\\detr\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoImageProcessor, DetrForObjectDetection\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
        "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
        "\n",
        "# Change model to eval mode and onto the GPU\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# Creating a function to infer images from a directory and return the predictions in a single coco eval json file\n",
        "def infer_images_from_directory(model, image_processor, img_directory, output_file, size=(3840, 2160)):\n",
        "    \"\"\"\n",
        "        Function to infer images from a directory and return the predictions in a single coco eval json file\n",
        "\n",
        "        Args:\n",
        "            model: The model to be used for inference\n",
        "            image_processor: The image processor\n",
        "            img_directory: The directory containing the images\n",
        "            output_file: The output file where the predictions will be saved\n",
        "            size: The size to which the images will be resized\n",
        "\n",
        "        Returns:\n",
        "            coco_output: The predictions in COCO eval json format\n",
        "    \"\"\"\n",
        "    # Loading the images from the directory\n",
        "    images = []\n",
        "    for img in os.listdir(img_directory):\n",
        "        images.append(img)\n",
        "\n",
        "    coco_annotations = []\n",
        "    coco_images = []\n",
        "\n",
        "    for image_name in images:\n",
        "        # Loading the image\n",
        "        image = Image.open(os.path.join(img_directory, image_name))\n",
        "        \n",
        "        inputs = image_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "        \n",
        "        # Move the inputs to the device and define by batch size\n",
        "        inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # converting outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n",
        "        target_sizes = torch.tensor([image.size[::-1]]).to(device)\n",
        "\n",
        "        results = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\n",
        "        \n",
        "        image_id = int(image_name.split('_')[-1].split('.')[0])\n",
        "\n",
        "        # Appending the image info to the coco_images list\n",
        "        coco_images.append({\n",
        "            'file_name': image_name,\n",
        "            'height': image.size[1],\n",
        "            'width': image.size[0],\n",
        "            'id': image_id\n",
        "        })\n",
        "\n",
        "        # Appending the predictions to the coco_annotations list\n",
        "        for i in range(len(results['labels'])):\n",
        "            coco_annotations.append({\n",
        "                'id': len(coco_annotations) + 1,\n",
        "                'image_id': image_id,\n",
        "                'category_id': int(results['labels'][i]),\n",
        "                'bbox': [float(results['boxes'][i][0]), float(results['boxes'][i][1]), float(results['boxes'][i][2] - results['boxes'][i][0]), float(results['boxes'][i][3] - results['boxes'][i][1])],\n",
        "                'score': float(results['scores'][i]),\n",
        "                'area': float(results['boxes'][i][2] * results['boxes'][i][3]),  # Assuming the area is width * height\n",
        "                'segmentation': [], # Assuming the segmentation is empty\n",
        "                'iscrowd': 0\n",
        "            })\n",
        "\n",
        "        if len(coco_images) == 100:  # Limiting the number of images to 100 \n",
        "            break\n",
        "    \n",
        "    # Removing annotations which don't have a category_id in the categories list\n",
        "    coco_annotations = [annotation for annotation in coco_annotations if annotation['category_id'] in [category['id'] for category in categories]]\n",
        "    \n",
        "    # Saving the predictions to COCO eval json file\n",
        "    coco_output = {\n",
        "        'categories': categories,  # Assuming 90 categories\n",
        "        'images': coco_images,\n",
        "        'annotations': coco_annotations,\n",
        "    }\n",
        "\n",
        "    # Creating directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "    \n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(coco_output, f, indent=4)\n",
        "\n",
        "    return coco_output\n",
        "\n",
        "# Example usage\n",
        "predictions_json = 'predictions/DETR/predictions_2160p.json'\n",
        "images_path = 'frames/2160p'\n",
        "size = (3840, 2160)\n",
        "\n",
        "start = time.time()\n",
        "infer_images_from_directory(model, image_processor, images_path, predictions_json, size)\n",
        "end = time.time()\n",
        "\n",
        "# Saving the time taken to infer the images\n",
        "with open('predictions/DETR/time_taken.txt', 'w') as f:\n",
        "    f.write(f'Time taken to infer images: {end - start} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Grounding DINO - Zero Shot Detection - Takes some time and not picking up cars</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['a bicycle a motorcycle', 'a bus'] 3\n",
            "['a bicycle a motorcycle', 'a bus'] 3\n",
            "['a bicycle a motorcycle', 'a bus'] 3\n",
            "['a bicycle a motorcycle', 'a bus'] 3\n",
            "['a motorcycle', 'a bus'] 3\n",
            "['a motorcycle', 'a bus'] 3\n",
            "['a bus', 'a a motorcycle'] 3\n",
            "['a bus', 'a a motorcycle'] 3\n",
            "['a bus', 'a bicycle a motorcycle'] 3\n",
            "['a bus', 'a bicycle a motorcycle'] 3\n",
            "['a bus', 'a bicycle a motorcycle'] 3\n",
            "['a bus', 'a bicycle a motorcycle'] 3\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 114\u001b[0m\n\u001b[0;32m    111\u001b[0m size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3840\u001b[39m, \u001b[38;5;241m2160\u001b[39m)\n\u001b[0;32m    113\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 114\u001b[0m \u001b[43minfer_images_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Saving the time taken to infer the images\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[3], line 46\u001b[0m, in \u001b[0;36minfer_images_from_directory\u001b[1;34m(model, image_processor, img_directory, output_file, size)\u001b[0m\n\u001b[0;32m     41\u001b[0m inputs \u001b[38;5;241m=\u001b[39m image_processor(images\u001b[38;5;241m=\u001b[39mimage, text\u001b[38;5;241m=\u001b[39mtext, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Move the inputs to the device and define by batch size\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# converting outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\u001b[39;00m\n\u001b[0;32m     49\u001b[0m target_sizes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([image\u001b[38;5;241m.\u001b[39msize[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;241m.\u001b[39mto(device)\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\detr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\detr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\detr\\lib\\site-packages\\transformers\\models\\grounding_dino\\modeling_grounding_dino.py:3016\u001b[0m, in \u001b[0;36mGroundingDinoForObjectDetection.forward\u001b[1;34m(self, pixel_values, input_ids, token_type_ids, attention_mask, pixel_mask, encoder_outputs, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[0;32m   3013\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(input_ids)\n\u001b[0;32m   3015\u001b[0m \u001b[38;5;66;03m# First, sent images through Grounding DINO base model to obtain encoder + decoder outputs\u001b[39;00m\n\u001b[1;32m-> 3016\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3018\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3023\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3024\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3026\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3028\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   3029\u001b[0m enc_text_hidden_state \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mencoder_last_hidden_state_text \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;28;01melse\u001b[39;00m outputs[idx]\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\detr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\detr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\detr\\lib\\site-packages\\transformers\\models\\grounding_dino\\modeling_grounding_dino.py:2353\u001b[0m, in \u001b[0;36mGroundingDinoModel.forward\u001b[1;34m(self, pixel_values, input_ids, token_type_ids, attention_mask, pixel_mask, encoder_outputs, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   2350\u001b[0m \u001b[38;5;66;03m# Fourth, sent source_flatten + mask_flatten + lvl_pos_embed_flatten (backbone + proj layer output) through encoder\u001b[39;00m\n\u001b[0;32m   2351\u001b[0m \u001b[38;5;66;03m# Also provide spatial_shapes, level_start_index and valid_ratios\u001b[39;00m\n\u001b[0;32m   2352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2353\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvision_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvision_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mmask_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvision_position_embedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlvl_pos_embed_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalid_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_ratios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mtext_token_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_position_embedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_self_attention_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mtext_self_attention_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_position_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2365\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2366\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2368\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2369\u001b[0m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a GroundingDinoEncoderOutput when return_dict=True\u001b[39;00m\n\u001b[0;32m   2370\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, GroundingDinoEncoderOutput):\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\detr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\detr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\detr\\lib\\site-packages\\transformers\\models\\grounding_dino\\modeling_grounding_dino.py:1746\u001b[0m, in \u001b[0;36mGroundingDinoEncoder.forward\u001b[1;34m(self, vision_features, vision_attention_mask, vision_position_embedding, spatial_shapes, level_start_index, valid_ratios, text_features, text_attention_mask, text_position_embedding, text_self_attention_masks, text_position_ids, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1743\u001b[0m     encoder_vision_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (vision_features,)\n\u001b[0;32m   1744\u001b[0m     encoder_text_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (text_features,)\n\u001b[1;32m-> 1746\u001b[0m (vision_features, text_features), attentions \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvision_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvision_position_embedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_position_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreference_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreference_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_position_embedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_position_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_self_attention_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_self_attention_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_position_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m   1761\u001b[0m     all_attn_fused_vision \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (attentions[\u001b[38;5;241m0\u001b[39m],)\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\detr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\detr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\detr\\lib\\site-packages\\transformers\\models\\grounding_dino\\modeling_grounding_dino.py:1273\u001b[0m, in \u001b[0;36mGroundingDinoEncoderLayer.forward\u001b[1;34m(self, vision_features, vision_position_embedding, spatial_shapes, level_start_index, key_padding_mask, reference_points, text_features, text_attention_mask, text_position_embedding, text_self_attention_masks, text_position_ids)\u001b[0m\n\u001b[0;32m   1260\u001b[0m (vision_features, vision_fused_attn), (text_features, text_fused_attn) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfusion_layer(\n\u001b[0;32m   1261\u001b[0m     vision_features\u001b[38;5;241m=\u001b[39mvision_features,\n\u001b[0;32m   1262\u001b[0m     text_features\u001b[38;5;241m=\u001b[39mtext_features,\n\u001b[0;32m   1263\u001b[0m     attention_mask_vision\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[0;32m   1264\u001b[0m     attention_mask_text\u001b[38;5;241m=\u001b[39mtext_attention_mask,\n\u001b[0;32m   1265\u001b[0m )\n\u001b[0;32m   1267\u001b[0m (text_features, text_enhanced_attn) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_enhancer_layer(\n\u001b[0;32m   1268\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mtext_features,\n\u001b[0;32m   1269\u001b[0m     attention_masks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m~\u001b[39mtext_self_attention_masks,  \u001b[38;5;66;03m# note we use ~ for mask here\u001b[39;00m\n\u001b[0;32m   1270\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39m(text_position_embedding \u001b[38;5;28;01mif\u001b[39;00m text_position_embedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1271\u001b[0m )\n\u001b[1;32m-> 1273\u001b[0m (vision_features, vision_deformable_attn) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeformable_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_position_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreference_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreference_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1280\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m   1283\u001b[0m     (vision_features, text_features),\n\u001b[0;32m   1284\u001b[0m     (vision_fused_attn, text_fused_attn, text_enhanced_attn, vision_deformable_attn),\n\u001b[0;32m   1285\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\detr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\detr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\detr\\lib\\site-packages\\transformers\\models\\grounding_dino\\modeling_grounding_dino.py:1139\u001b[0m, in \u001b[0;36mGroundingDinoDeformableLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_embeddings, reference_points, spatial_shapes, level_start_index, output_attentions)\u001b[0m\n\u001b[0;32m   1136\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# Apply Multi-scale Deformable Attention Module on the multi-scale feature maps.\u001b[39;00m\n\u001b[1;32m-> 1139\u001b[0m hidden_states, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreference_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreference_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1151\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m   1152\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\detr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\detr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\detr\\lib\\site-packages\\transformers\\models\\grounding_dino\\modeling_grounding_dino.py:704\u001b[0m, in \u001b[0;36mGroundingDinoMultiscaleDeformableAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, position_embeddings, reference_points, spatial_shapes, level_start_index, output_attentions)\u001b[0m\n\u001b[0;32m    702\u001b[0m batch_size, num_queries, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    703\u001b[0m batch_size, sequence_length, _ \u001b[38;5;241m=\u001b[39m encoder_hidden_states\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure to align the spatial shapes with the sequence length of the encoder hidden states\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    707\u001b[0m     )\n\u001b[0;32m    709\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_proj(encoder_hidden_states)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
        "\n",
        "model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id)\n",
        "\n",
        "# Change model to eval mode and onto the GPU\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# Creating a function to infer images from a directory and return the predictions in a single coco eval json file\n",
        "def infer_images_from_directory(model, image_processor, img_directory, output_file, size=(3840, 2160)):\n",
        "    \"\"\"\n",
        "        Function to infer images from a directory and return the predictions in a single coco eval json file\n",
        "\n",
        "        Args:\n",
        "            model: The model to be used for inference\n",
        "            image_processor: The image processor\n",
        "            img_directory: The directory containing the images\n",
        "            output_file: The output file where the predictions will be saved\n",
        "            size: The size to which the images will be resized\n",
        "\n",
        "        Returns:\n",
        "            coco_output: The predictions in COCO eval json format\n",
        "    \"\"\"\n",
        "    # Loading the images from the directory\n",
        "    images = []\n",
        "    for img in os.listdir(img_directory):\n",
        "        images.append(img)\n",
        "\n",
        "    coco_annotations = []\n",
        "    coco_images = []\n",
        "\n",
        "    text =\" a person. a bicycle. a car. a motorcycle. a bus. a truck.\"\n",
        "\n",
        "    for image_name in images:\n",
        "        # Loading the image\n",
        "        image = Image.open(os.path.join(img_directory, image_name))\n",
        "        \n",
        "        inputs = image_processor(images=image, text=text, return_tensors=\"pt\").to(device)\n",
        "        \n",
        "        # Move the inputs to the device and define by batch size\n",
        "        # inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # converting outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n",
        "        target_sizes = torch.tensor([image.size[::-1]]).to(device)\n",
        "\n",
        "        results = image_processor.post_process_grounded_object_detection(outputs, inputs.input_ids, box_threshold=0.4, text_threshold=0.3, target_sizes=target_sizes)[0]\n",
        "        \n",
        "        image_id = int(image_name.split('_')[-1].split('.')[0])\n",
        "\n",
        "        # Appending the image info to the coco_images list\n",
        "        coco_images.append({\n",
        "            'file_name': image_name,\n",
        "            'height': image.size[1],\n",
        "            'width': image.size[0],\n",
        "            'id': image_id\n",
        "        })\n",
        "\n",
        "        # Create a mapping from string labels to integer category IDs\n",
        "        label_to_category_id = {category['name']: category['id'] for category in categories}\n",
        "\n",
        "        # Appending the predictions to the coco_annotations list\n",
        "        for i in range(len(results['labels'])):\n",
        "            # Retrieving initial category_id from the labels\n",
        "            category_id = label_to_category_id['car']  # Default category_id\n",
        "            # Search through the labels whether there is a token that is in the label\n",
        "            for label in results['labels'][i]:\n",
        "                if label in label_to_category_id:\n",
        "                    category_id = label_to_category_id[label]\n",
        "                    break\n",
        "            print(results['labels'], category_id)\n",
        "            coco_annotations.append({\n",
        "                'id': len(coco_annotations) + 1,\n",
        "                'image_id': image_id,\n",
        "                'category_id': category_id,\n",
        "                'bbox': [float(results['boxes'][i][0]), float(results['boxes'][i][1]), float(results['boxes'][i][2] - results['boxes'][i][0]), float(results['boxes'][i][3] - results['boxes'][i][1])],\n",
        "                'score': float(results['scores'][i]),\n",
        "                'area': float(results['boxes'][i][2] * results['boxes'][i][3]),  # Assuming the area is width * height\n",
        "                'segmentation': [], # Assuming the segmentation is empty\n",
        "                'iscrowd': 0\n",
        "            })\n",
        "\n",
        "        if len(coco_images) == 100:  # Limiting the number of images to 100 \n",
        "            break\n",
        "    \n",
        "    # Removing annotations which don't have a category_id in the categories list\n",
        "    coco_annotations = [annotation for annotation in coco_annotations if annotation['category_id'] in [category['id'] for category in categories]]\n",
        "    \n",
        "    # Saving the predictions to COCO eval json file\n",
        "    coco_output = {\n",
        "        'categories': categories,  # Assuming 90 categories\n",
        "        'images': coco_images,\n",
        "        'annotations': coco_annotations,\n",
        "    }\n",
        "\n",
        "    # Creating directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "    \n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(coco_output, f, indent=4)\n",
        "\n",
        "    return coco_output\n",
        "\n",
        "# Example usage\n",
        "predictions_json = 'predictions/GroundingDINO/predictions_2160p.json'\n",
        "images_path = 'frames/2160p'\n",
        "size = (3840, 2160)\n",
        "\n",
        "start = time.time()\n",
        "infer_images_from_directory(model, processor, images_path, predictions_json, size)\n",
        "end = time.time()\n",
        "\n",
        "# Saving the time taken to infer the images\n",
        "with open('predictions/GroundingDINO/time_taken.txt', 'w') as f:\n",
        "    f.write(f'Time taken to infer images: {end - start} seconds')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>RetinaNet</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading the retinanet model from pytorch hub\n",
        "import torchvision\n",
        "from torchvision.models.detection.retinanet import RetinaNet\n",
        "from torchvision.models.detection.retinanet import RetinaNet_ResNet50_FPN_Weights\n",
        "\n",
        "model = torchvision.models.detection.retinanet_resnet50_fpn(weights=RetinaNet_ResNet50_FPN_Weights.DEFAULT)\n",
        "model.eval()\n",
        "\n",
        "# model.to(device)\n",
        "\n",
        "# Defining a transform to resize the image\n",
        "transform = torchvision.transforms.Compose([\n",
        "    # torchvision.transforms.Resize((800, 800)),\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "# Creating a function to infer images from a directory and return the predictions in a single coco eval json file\n",
        "def pytorch_infer_images_from_directory(model, transform_function, img_directory, output_file):\n",
        "    \"\"\"\n",
        "        Function to infer images from a directory and return the predictions in a single coco eval json file\n",
        "\n",
        "        Args:\n",
        "            model: The model to be used for inference\n",
        "            transform_function: The transform function to be applied to the images\n",
        "            img_directory: The directory containing the images\n",
        "            output_file: The output file where the predictions will be saved\n",
        "\n",
        "        Returns:\n",
        "            coco_output: The predictions in COCO eval json format\n",
        "    \"\"\"\n",
        "    # Loading the images from the directory\n",
        "    images = []\n",
        "    for img in os.listdir(img_directory):\n",
        "        images.append(img)\n",
        "\n",
        "    coco_annotations = []\n",
        "    coco_images = []\n",
        "\n",
        "    for image_name in images:\n",
        "        # Loading the image\n",
        "        image = Image.open(os.path.join(img_directory, image_name))\n",
        "        old_image = image.copy()\n",
        "\n",
        "        # Applying the transform to the image\n",
        "        image = transform_function(image)\n",
        "\n",
        "        inputs = [image]\n",
        "\n",
        "        # inputs = list(img.to(device) for img in image)\n",
        "\n",
        "        # Getting the predictions\n",
        "        with torch.no_grad():\n",
        "            predictions = model(inputs)\n",
        "            results = predictions[0]\n",
        "\n",
        "        image_id = int(image_name.split('_')[-1].split('.')[0])\n",
        "\n",
        "        # Removing predictions with score less than threshold\n",
        "        threshold = 0.5\n",
        "        results = {key: value[results['scores'] > threshold] for key, value in results.items()}\n",
        "\n",
        "        # Appending the image info to the coco_images list\n",
        "        coco_images.append({\n",
        "            'file_name': image_name,\n",
        "            'height': old_image.size[1],\n",
        "            'width': old_image.size[0],\n",
        "            'id': image_id\n",
        "        })\n",
        "\n",
        "        # Appending the predictions to the coco_annotations list\n",
        "        for i in range(len(results['labels'])):\n",
        "            coco_annotations.append({\n",
        "                'id': len(coco_annotations) + 1,\n",
        "                'image_id': image_id,\n",
        "                'category_id': int(results['labels'][i]),\n",
        "                'bbox': [float(results['boxes'][i][0]), float(results['boxes'][i][1]), float(results['boxes'][i][2] - results['boxes'][i][0]), float(results['boxes'][i][3] - results['boxes'][i][1])],\n",
        "                'score': float(results['scores'][i]),\n",
        "                'area': float((results['boxes'][i][2] - results['boxes'][i][0]) * (results['boxes'][i][3] - results['boxes'][i][1]).item()),  # Assuming the bbox format is [xmin, ymin, xmax, ymax]                'iscrowd': 0\n",
        "                'segmentation': [], # Assuming the segmentation is empty\n",
        "                'iscrowd': 0\n",
        "            })\n",
        "\n",
        "        if len(coco_images) == 100:  # Limiting the number of images to 100 \n",
        "            break\n",
        "    \n",
        "    # Removing annotations which don't have a category_id in the categories list\n",
        "    coco_annotations = [annotation for annotation in coco_annotations if annotation['category_id'] in [category['id'] for category in categories]]\n",
        "    \n",
        "    # Saving the predictions to COCO eval json file\n",
        "    coco_output = {\n",
        "        'categories': categories,  # Assuming 90 categories\n",
        "        'images': coco_images,\n",
        "        'annotations': coco_annotations,\n",
        "    }\n",
        "\n",
        "    # Creating directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "    \n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(coco_output, f, indent=4)\n",
        "\n",
        "    return coco_output\n",
        "\n",
        "# Example usage\n",
        "predictions_json = 'predictions/RetinaNet/predictions_2160p.json'\n",
        "images_path = 'frames/2160p'\n",
        "\n",
        "start = time.time()\n",
        "pytorch_infer_images_from_directory(model, transform, images_path, predictions_json)\n",
        "end = time.time()\n",
        "\n",
        "# Saving the time taken to infer the images\n",
        "with open('predictions/RetinaNet/time_taken.txt', 'w') as f:\n",
        "    f.write(f'Time taken to infer images: {end - start} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Faster R-CNN</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\anaconda3\\envs\\detr\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Loading the retinanet model from pytorch hub\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
        "from torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_Weights\n",
        "\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
        "model.eval()\n",
        "\n",
        "# model.to(device)\n",
        "\n",
        "# Defining a transform to resize the image\n",
        "transform = torchvision.transforms.Compose([\n",
        "    # torchvision.transforms.Resize((800, 800)),\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "# Creating a function to infer images from a directory and return the predictions in a single coco eval json file\n",
        "def pytorch_infer_images_from_directory(model, transform_function, img_directory, output_file):\n",
        "    \"\"\"\n",
        "        Function to infer images from a directory and return the predictions in a single coco eval json file\n",
        "\n",
        "        Args:\n",
        "            model: The model to be used for inference\n",
        "            transform_function: The transform function to be applied to the images\n",
        "            img_directory: The directory containing the images\n",
        "            output_file: The output file where the predictions will be saved\n",
        "\n",
        "        Returns:\n",
        "            coco_output: The predictions in COCO eval json format\n",
        "    \"\"\"\n",
        "    # Loading the images from the directory\n",
        "    images = []\n",
        "    for img in os.listdir(img_directory):\n",
        "        images.append(img)\n",
        "\n",
        "    coco_annotations = []\n",
        "    coco_images = []\n",
        "\n",
        "    for image_name in images:\n",
        "        # Loading the image\n",
        "        image = Image.open(os.path.join(img_directory, image_name))\n",
        "        old_image = image.copy()\n",
        "\n",
        "        # Applying the transform to the image\n",
        "        image = transform_function(image)\n",
        "\n",
        "        inputs = [image]\n",
        "\n",
        "        # inputs = list(img.to(device) for img in image)\n",
        "\n",
        "        # Getting the predictions\n",
        "        with torch.no_grad():\n",
        "            predictions = model(inputs)\n",
        "            results = predictions[0]\n",
        "\n",
        "        image_id = int(image_name.split('_')[-1].split('.')[0])\n",
        "\n",
        "        # Removing predictions with score less than threshold\n",
        "        threshold = 0.5\n",
        "        results = {key: value[results['scores'] > threshold] for key, value in results.items()}\n",
        "\n",
        "        # Appending the image info to the coco_images list\n",
        "        coco_images.append({\n",
        "            'file_name': image_name,\n",
        "            'height': old_image.size[1],\n",
        "            'width': old_image.size[0],\n",
        "            'id': image_id\n",
        "        })\n",
        "\n",
        "        # Appending the predictions to the coco_annotations list\n",
        "        for i in range(len(results['labels'])):\n",
        "            coco_annotations.append({\n",
        "                'id': len(coco_annotations) + 1,\n",
        "                'image_id': image_id,\n",
        "                'category_id': int(results['labels'][i]),\n",
        "                'bbox': [float(results['boxes'][i][0]), float(results['boxes'][i][1]), float(results['boxes'][i][2] - results['boxes'][i][0]), float(results['boxes'][i][3] - results['boxes'][i][1])],\n",
        "                'score': float(results['scores'][i]),\n",
        "                'area': float((results['boxes'][i][2] - results['boxes'][i][0]) * (results['boxes'][i][3] - results['boxes'][i][1]).item()),  # Assuming the bbox format is [xmin, ymin, xmax, ymax]                'iscrowd': 0\n",
        "                'segmentation': [], # Assuming the segmentation is empty\n",
        "                'iscrowd': 0\n",
        "            })\n",
        "\n",
        "        if len(coco_images) == 100:  # Limiting the number of images to 100 \n",
        "            break\n",
        "    \n",
        "    # Removing annotations which don't have a category_id in the categories list\n",
        "    coco_annotations = [annotation for annotation in coco_annotations if annotation['category_id'] in [category['id'] for category in categories]]\n",
        "    \n",
        "    # Saving the predictions to COCO eval json file\n",
        "    coco_output = {\n",
        "        'categories': categories,  # Assuming 90 categories\n",
        "        'images': coco_images,\n",
        "        'annotations': coco_annotations,\n",
        "    }\n",
        "\n",
        "    # Creating directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "    \n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(coco_output, f, indent=4)\n",
        "\n",
        "    return coco_output\n",
        "\n",
        "# Example usage\n",
        "predictions_json = 'predictions/FasterRCNN/predictions_2160p.json'\n",
        "images_path = 'frames/2160p'\n",
        "\n",
        "start = time.time()\n",
        "pytorch_infer_images_from_directory(model, transform, images_path, predictions_json)\n",
        "end = time.time()\n",
        "\n",
        "# Saving the time taken to infer the images\n",
        "with open('predictions/FasterRCNN/time_taken.txt', 'w') as f:\n",
        "    f.write(f'Time taken to infer images: {end - start} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>FCOS</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fcos_resnet50_fpn_coco-99b0c9b7.pth\" to C:\\Users\\User/.cache\\torch\\hub\\checkpoints\\fcos_resnet50_fpn_coco-99b0c9b7.pth\n",
            "100%|██████████| 124M/124M [00:19<00:00, 6.57MB/s] \n"
          ]
        }
      ],
      "source": [
        "# Loading the retinanet model from pytorch hub\n",
        "import torchvision\n",
        "from torchvision.models.detection.fcos import FCOS\n",
        "from torchvision.models.detection.fcos import FCOS_ResNet50_FPN_Weights\n",
        "\n",
        "model = torchvision.models.detection.fcos_resnet50_fpn(weights=FCOS_ResNet50_FPN_Weights.DEFAULT)\n",
        "model.eval()\n",
        "\n",
        "# model.to(device)\n",
        "\n",
        "# Defining a transform to resize the image\n",
        "transform = torchvision.transforms.Compose([\n",
        "    # torchvision.transforms.Resize((800, 800)),\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "# Creating a function to infer images from a directory and return the predictions in a single coco eval json file\n",
        "def pytorch_infer_images_from_directory(model, transform_function, img_directory, output_file):\n",
        "    \"\"\"\n",
        "        Function to infer images from a directory and return the predictions in a single coco eval json file\n",
        "\n",
        "        Args:\n",
        "            model: The model to be used for inference\n",
        "            transform_function: The transform function to be applied to the images\n",
        "            img_directory: The directory containing the images\n",
        "            output_file: The output file where the predictions will be saved\n",
        "\n",
        "        Returns:\n",
        "            coco_output: The predictions in COCO eval json format\n",
        "    \"\"\"\n",
        "    # Loading the images from the directory\n",
        "    images = []\n",
        "    for img in os.listdir(img_directory):\n",
        "        images.append(img)\n",
        "\n",
        "    coco_annotations = []\n",
        "    coco_images = []\n",
        "\n",
        "    for image_name in images:\n",
        "        # Loading the image\n",
        "        image = Image.open(os.path.join(img_directory, image_name))\n",
        "        old_image = image.copy()\n",
        "\n",
        "        # Applying the transform to the image\n",
        "        image = transform_function(image)\n",
        "\n",
        "        inputs = [image]\n",
        "\n",
        "        # inputs = list(img.to(device) for img in image)\n",
        "\n",
        "        # Getting the predictions\n",
        "        with torch.no_grad():\n",
        "            predictions = model(inputs)\n",
        "            results = predictions[0]\n",
        "\n",
        "        image_id = int(image_name.split('_')[-1].split('.')[0])\n",
        "\n",
        "        # Removing predictions with score less than threshold\n",
        "        threshold = 0.5\n",
        "        results = {key: value[results['scores'] > threshold] for key, value in results.items()}\n",
        "\n",
        "        # Appending the image info to the coco_images list\n",
        "        coco_images.append({\n",
        "            'file_name': image_name,\n",
        "            'height': old_image.size[1],\n",
        "            'width': old_image.size[0],\n",
        "            'id': image_id\n",
        "        })\n",
        "\n",
        "        # Appending the predictions to the coco_annotations list\n",
        "        for i in range(len(results['labels'])):\n",
        "            coco_annotations.append({\n",
        "                'id': len(coco_annotations) + 1,\n",
        "                'image_id': image_id,\n",
        "                'category_id': int(results['labels'][i]),\n",
        "                'bbox': [float(results['boxes'][i][0]), float(results['boxes'][i][1]), float(results['boxes'][i][2] - results['boxes'][i][0]), float(results['boxes'][i][3] - results['boxes'][i][1])],\n",
        "                'score': float(results['scores'][i]),\n",
        "                'area': float((results['boxes'][i][2] - results['boxes'][i][0]) * (results['boxes'][i][3] - results['boxes'][i][1]).item()),  # Assuming the bbox format is [xmin, ymin, xmax, ymax]                'iscrowd': 0\n",
        "                'segmentation': [], # Assuming the segmentation is empty\n",
        "                'iscrowd': 0\n",
        "            })\n",
        "\n",
        "        if len(coco_images) == 100:  # Limiting the number of images to 100 \n",
        "            break\n",
        "    \n",
        "    # Removing annotations which don't have a category_id in the categories list\n",
        "    coco_annotations = [annotation for annotation in coco_annotations if annotation['category_id'] in [category['id'] for category in categories]]\n",
        "    \n",
        "    # Saving the predictions to COCO eval json file\n",
        "    coco_output = {\n",
        "        'categories': categories,  # Assuming 90 categories\n",
        "        'images': coco_images,\n",
        "        'annotations': coco_annotations,\n",
        "    }\n",
        "\n",
        "    # Creating directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "    \n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(coco_output, f, indent=4)\n",
        "\n",
        "    return coco_output\n",
        "\n",
        "# Example usage\n",
        "predictions_json = 'predictions/FCOS/predictions_2160p.json'\n",
        "images_path = 'frames/2160p'\n",
        "\n",
        "start = time.time()\n",
        "pytorch_infer_images_from_directory(model, transform, images_path, predictions_json)\n",
        "end = time.time()\n",
        "\n",
        "# Saving the time taken to infer the images\n",
        "with open('predictions/FCOS/time_taken.txt', 'w') as f:\n",
        "    f.write(f'Time taken to infer images: {end - start} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>SSD</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading the retinanet model from pytorch hub\n",
        "import torchvision\n",
        "from torchvision.models.detection.ssd import SSD\n",
        "from torchvision.models.detection.ssd import SSD300_VGG16_Weights\n",
        "\n",
        "model = torchvision.models.detection.ssd300_vgg16(weights=SSD300_VGG16_Weights.DEFAULT)\n",
        "model.eval()\n",
        "\n",
        "# model.to(device)\n",
        "\n",
        "# Defining a transform to resize the image\n",
        "transform = torchvision.transforms.Compose([\n",
        "    # torchvision.transforms.Resize((800, 800)),\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "# Creating a function to infer images from a directory and return the predictions in a single coco eval json file\n",
        "def pytorch_infer_images_from_directory(model, transform_function, img_directory, output_file):\n",
        "    \"\"\"\n",
        "        Function to infer images from a directory and return the predictions in a single coco eval json file\n",
        "\n",
        "        Args:\n",
        "            model: The model to be used for inference\n",
        "            transform_function: The transform function to be applied to the images\n",
        "            img_directory: The directory containing the images\n",
        "            output_file: The output file where the predictions will be saved\n",
        "\n",
        "        Returns:\n",
        "            coco_output: The predictions in COCO eval json format\n",
        "    \"\"\"\n",
        "    # Loading the images from the directory\n",
        "    images = []\n",
        "    for img in os.listdir(img_directory):\n",
        "        images.append(img)\n",
        "\n",
        "    coco_annotations = []\n",
        "    coco_images = []\n",
        "\n",
        "    for image_name in images:\n",
        "        # Loading the image\n",
        "        image = Image.open(os.path.join(img_directory, image_name))\n",
        "        old_image = image.copy()\n",
        "\n",
        "        # Applying the transform to the image\n",
        "        image = transform_function(image)\n",
        "\n",
        "        inputs = [image]\n",
        "\n",
        "        # inputs = list(img.to(device) for img in image)\n",
        "\n",
        "        # Getting the predictions\n",
        "        with torch.no_grad():\n",
        "            predictions = model(inputs)\n",
        "            results = predictions[0]\n",
        "\n",
        "        image_id = int(image_name.split('_')[-1].split('.')[0])\n",
        "\n",
        "        # Removing predictions with score less than threshold\n",
        "        threshold = 0.3\n",
        "        results = {key: value[results['scores'] > threshold] for key, value in results.items()}\n",
        "\n",
        "        # Appending the image info to the coco_images list\n",
        "        coco_images.append({\n",
        "            'file_name': image_name,\n",
        "            'height': old_image.size[1],\n",
        "            'width': old_image.size[0],\n",
        "            'id': image_id\n",
        "        })\n",
        "\n",
        "        # Appending the predictions to the coco_annotations list\n",
        "        for i in range(len(results['labels'])):\n",
        "            coco_annotations.append({\n",
        "                'id': len(coco_annotations) + 1,\n",
        "                'image_id': image_id,\n",
        "                'category_id': int(results['labels'][i]),\n",
        "                'bbox': [float(results['boxes'][i][0]), float(results['boxes'][i][1]), float(results['boxes'][i][2] - results['boxes'][i][0]), float(results['boxes'][i][3] - results['boxes'][i][1])],\n",
        "                'score': float(results['scores'][i]),\n",
        "                'area': float((results['boxes'][i][2] - results['boxes'][i][0]) * (results['boxes'][i][3] - results['boxes'][i][1]).item()),  # Assuming the bbox format is [xmin, ymin, xmax, ymax]                'iscrowd': 0\n",
        "                'segmentation': [], # Assuming the segmentation is empty\n",
        "                'iscrowd': 0\n",
        "            })\n",
        "\n",
        "        if len(coco_images) == 100:  # Limiting the number of images to 100 \n",
        "            break\n",
        "    \n",
        "    # Removing annotations which don't have a category_id in the categories list\n",
        "    coco_annotations = [annotation for annotation in coco_annotations if annotation['category_id'] in [category['id'] for category in categories]]\n",
        "    \n",
        "    # Saving the predictions to COCO eval json file\n",
        "    coco_output = {\n",
        "        'categories': categories,  # Assuming 90 categories\n",
        "        'images': coco_images,\n",
        "        'annotations': coco_annotations,\n",
        "    }\n",
        "\n",
        "    # Creating directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "    \n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(coco_output, f, indent=4)\n",
        "\n",
        "    return coco_output\n",
        "\n",
        "# Example usage\n",
        "predictions_json = 'predictions/SSD/predictions_2160p.json'\n",
        "images_path = 'frames/2160p'\n",
        "\n",
        "start = time.time()\n",
        "pytorch_infer_images_from_directory(model, transform, images_path, predictions_json)\n",
        "end = time.time()\n",
        "\n",
        "# Saving the time taken to infer the images\n",
        "with open('predictions/SSD/time_taken.txt', 'w') as f:\n",
        "    f.write(f'Time taken to infer images: {end - start} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>SSDLite</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading the retinanet model from pytorch hub\n",
        "import torchvision\n",
        "# from torchvision.models.detection.ssdlite import SSDLite\n",
        "from torchvision.models.detection.ssdlite import SSDLite320_MobileNet_V3_Large_Weights\n",
        "\n",
        "model = torchvision.models.detection.ssdlite320_mobilenet_v3_large(weights=SSDLite320_MobileNet_V3_Large_Weights.DEFAULT)\n",
        "model.eval()\n",
        "\n",
        "# model.to(device)\n",
        "\n",
        "# Defining a transform to resize the image\n",
        "transform = torchvision.transforms.Compose([\n",
        "    # torchvision.transforms.Resize((800, 800)),\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "# Creating a function to infer images from a directory and return the predictions in a single coco eval json file\n",
        "def pytorch_infer_images_from_directory(model, transform_function, img_directory, output_file):\n",
        "    \"\"\"\n",
        "        Function to infer images from a directory and return the predictions in a single coco eval json file\n",
        "\n",
        "        Args:\n",
        "            model: The model to be used for inference\n",
        "            transform_function: The transform function to be applied to the images\n",
        "            img_directory: The directory containing the images\n",
        "            output_file: The output file where the predictions will be saved\n",
        "\n",
        "        Returns:\n",
        "            coco_output: The predictions in COCO eval json format\n",
        "    \"\"\"\n",
        "    # Loading the images from the directory\n",
        "    images = []\n",
        "    for img in os.listdir(img_directory):\n",
        "        images.append(img)\n",
        "\n",
        "    coco_annotations = []\n",
        "    coco_images = []\n",
        "\n",
        "    for image_name in images:\n",
        "        # Loading the image\n",
        "        image = Image.open(os.path.join(img_directory, image_name))\n",
        "        old_image = image.copy()\n",
        "\n",
        "        # Applying the transform to the image\n",
        "        image = transform_function(image)\n",
        "\n",
        "        inputs = [image]\n",
        "\n",
        "        # inputs = list(img.to(device) for img in image)\n",
        "\n",
        "        # Getting the predictions\n",
        "        with torch.no_grad():\n",
        "            predictions = model(inputs)\n",
        "            results = predictions[0]\n",
        "\n",
        "        image_id = int(image_name.split('_')[-1].split('.')[0])\n",
        "\n",
        "        # Removing predictions with score less than threshold\n",
        "        threshold = 0.3\n",
        "        results = {key: value[results['scores'] > threshold] for key, value in results.items()}\n",
        "\n",
        "        # Appending the image info to the coco_images list\n",
        "        coco_images.append({\n",
        "            'file_name': image_name,\n",
        "            'height': old_image.size[1],\n",
        "            'width': old_image.size[0],\n",
        "            'id': image_id\n",
        "        })\n",
        "\n",
        "        # Appending the predictions to the coco_annotations list\n",
        "        for i in range(len(results['labels'])):\n",
        "            coco_annotations.append({\n",
        "                'id': len(coco_annotations) + 1,\n",
        "                'image_id': image_id,\n",
        "                'category_id': int(results['labels'][i]),\n",
        "                'bbox': [float(results['boxes'][i][0]), float(results['boxes'][i][1]), float(results['boxes'][i][2] - results['boxes'][i][0]), float(results['boxes'][i][3] - results['boxes'][i][1])],\n",
        "                'score': float(results['scores'][i]),\n",
        "                'area': float((results['boxes'][i][2] - results['boxes'][i][0]) * (results['boxes'][i][3] - results['boxes'][i][1]).item()),  # Assuming the bbox format is [xmin, ymin, xmax, ymax]                'iscrowd': 0\n",
        "                'segmentation': [], # Assuming the segmentation is empty\n",
        "                'iscrowd': 0\n",
        "            })\n",
        "\n",
        "        if len(coco_images) == 100:  # Limiting the number of images to 100 \n",
        "            break\n",
        "    \n",
        "    # Removing annotations which don't have a category_id in the categories list\n",
        "    coco_annotations = [annotation for annotation in coco_annotations if annotation['category_id'] in [category['id'] for category in categories]]\n",
        "    \n",
        "    # Saving the predictions to COCO eval json file\n",
        "    coco_output = {\n",
        "        'categories': categories,  # Assuming 90 categories\n",
        "        'images': coco_images,\n",
        "        'annotations': coco_annotations,\n",
        "    }\n",
        "\n",
        "    # Creating directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "    \n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(coco_output, f, indent=4)\n",
        "\n",
        "    return coco_output\n",
        "\n",
        "# Example usage\n",
        "predictions_json = 'predictions/SSDLite/predictions_2160p.json'\n",
        "images_path = 'frames/2160p'\n",
        "\n",
        "start = time.time()\n",
        "pytorch_infer_images_from_directory(model, transform, images_path, predictions_json)\n",
        "end = time.time()\n",
        "\n",
        "# Saving the time taken to infer the images\n",
        "with open('predictions/SSDLite/time_taken.txt', 'w') as f:\n",
        "    f.write(f'Time taken to infer images: {end - start} seconds')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "196a6911458b41d9a3f8098747eadbe2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2775e4003dc14f489b2ca7765e308d1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd02f0dc988b4dec86de24c6de8fd590",
            "max": 71,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_95461069b82e491cb480ae290b01e4a0",
            "value": 71
          }
        },
        "31cf29ed51ab4442bf9e9a1ede11691e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_710b58f8127a44f4b02bc0c6c9f11923",
              "IPY_MODEL_b829d06e121e4d56ad3c6f88af47602a",
              "IPY_MODEL_dd5963a5d9044b71a1828ed767a13cf3"
            ],
            "layout": "IPY_MODEL_196a6911458b41d9a3f8098747eadbe2"
          }
        },
        "37af343cb4e347d8b562af3828d9392e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4929d632a43a4b39842bf43b28b2c60e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ec684f9d0d184ba59e84e68c61ec4724",
              "IPY_MODEL_2775e4003dc14f489b2ca7765e308d1e",
              "IPY_MODEL_6742f868d96b4cd6a57626882101fb41"
            ],
            "layout": "IPY_MODEL_d7a07fc02d524ffb8ead5d2974ac3daf"
          }
        },
        "6742f868d96b4cd6a57626882101fb41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92d949d3d38c4ae2b98561581a856dde",
            "placeholder": "​",
            "style": "IPY_MODEL_b0372c7ec0f9493282d3c412b98a0a3d",
            "value": " 71/71 [00:46&lt;00:00,  1.84it/s]"
          }
        },
        "6e6ce6b08fc246219d3a5379923e2c84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "710b58f8127a44f4b02bc0c6c9f11923": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a847feff1c11462c8a587c6d6319f7ad",
            "placeholder": "​",
            "style": "IPY_MODEL_a2f6b22f013e422da93706d01168d488",
            "value": "  0%"
          }
        },
        "92d949d3d38c4ae2b98561581a856dde": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95461069b82e491cb480ae290b01e4a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2f6b22f013e422da93706d01168d488": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a847feff1c11462c8a587c6d6319f7ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0372c7ec0f9493282d3c412b98a0a3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b829d06e121e4d56ad3c6f88af47602a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37af343cb4e347d8b562af3828d9392e",
            "max": 71,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e6ce6b08fc246219d3a5379923e2c84",
            "value": 0
          }
        },
        "c0e3b73a0d934cc1a339f39d9cb87840": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb2e2d9379ce4e0b829e85f9c21e3da3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd02f0dc988b4dec86de24c6de8fd590": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7a07fc02d524ffb8ead5d2974ac3daf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd5963a5d9044b71a1828ed767a13cf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb2e2d9379ce4e0b829e85f9c21e3da3",
            "placeholder": "​",
            "style": "IPY_MODEL_c0e3b73a0d934cc1a339f39d9cb87840",
            "value": " 0/71 [00:27&lt;?, ?it/s]"
          }
        },
        "e112386fc94543d19303892d790af67a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec684f9d0d184ba59e84e68c61ec4724": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecc24f45d64448c9a73fea1b4447052b",
            "placeholder": "​",
            "style": "IPY_MODEL_e112386fc94543d19303892d790af67a",
            "value": "100%"
          }
        },
        "ecc24f45d64448c9a73fea1b4447052b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
